{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input-Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "from scipy import interpolate\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Directory: 'C:\\Users\\Terabix\\Google_Drive\\Python_Coding\\' \n",
      "  Library path: 'C:\\Users\\Terabix\\Google_Drive\\Python_Coding\\Library\\' \n"
     ]
    }
   ],
   "source": [
    "while not os.getcwd().endswith('Python_Coding'):\n",
    "    # move one up until the directory folder is 'Python_Coding'\n",
    "    os.chdir('..')\n",
    "    if os.getcwd().endswith(':\\\\'):\n",
    "        break\n",
    "\n",
    "if os.getcwd().endswith('Python_Coding'):\n",
    "    Directory_address = os.getcwd() +'\\\\'\n",
    "    Library_address = Directory_address + 'Library\\\\'\n",
    "    print('Root Directory: \\'%s\\' \\n  Library path: \\'%s\\' ' %(Directory_address, Library_address) )\n",
    "else : \n",
    "    raise ValueError(\"Could not find the path of 'Python_Coding' folder \")\n",
    "\n",
    "input_folder_address = Directory_address + \"Inputs\\\\\"\n",
    "output_folder_address = Directory_address + \"Outputs\\\\\"\n",
    "\n",
    "def_file_name= \"data\"\n",
    "file_labels = \"\"\n",
    "data_type= \".txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'folder_address' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ce13169eadcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# file_labels can be used to import multiple files and assign the data to dictionaries, or concatenate them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m def import_data(folder_address=folder_address, file_name=def_file_name ,file_labels=file_labels, data_type= \".txt\", delimiter=',', header_rows=3, skip_columns=-1,\n\u001b[0m\u001b[0;32m      3\u001b[0m                 \u001b[0mnumeric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# importing all numeric data, they can be converted to float\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                \u001b[0mconcatenate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;31m# concatenate the files imported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                ):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'folder_address' is not defined"
     ]
    }
   ],
   "source": [
    "# file_labels can be used to import multiple files and assign the data to dictionaries, or concatenate them\n",
    "def import_data(input_folder_address=input_folder_address, file_name=def_file_name ,file_labels=file_labels, data_type= \".txt\", delimiter=',', header_rows=3, skip_columns=-1,\n",
    "                numeric = 1, # importing all numeric data, they can be converted to float\n",
    "               concatenate=1 # concatenate the files imported\n",
    "               ):\n",
    "    \n",
    "    # Detect the material, layer no, substrate (makes sure that re.findall returns something)\n",
    "    # detect the thickness such as 3 from 'WSe2_3L_A6_2018-02-19'\n",
    "    layer_no_regex = re.compile(r'_\\dL_')\n",
    "    \n",
    "    layer_no = int(re.findall( '_(.*?)L_' , layer_no_regex.findall(file_name)[0])[0]) if layer_no_regex.findall(file_name) else ''\n",
    "    thickness = str(layer_no)+'L' # make 2 -> 2L\n",
    "    # detect the material such as 'WSe2' from 'WSe2_2L_A6_2018-02-19'\n",
    "    material = re.findall('(.*?2)_', file_name)[0] if re.findall('(.*?2)_', file_name) else ''\n",
    "    # date of the experiment (finds 2018-05-30 or 2018_05_30)\n",
    "    date = re.findall('(?:201\\d_\\d\\d_\\d\\d|201\\d-\\d\\d-\\d\\d)', file_name)[0] if re.findall('(?:201\\d_\\d\\d_\\d\\d|201\\d-\\d\\d-\\d\\d)', file_name) else ''\n",
    "    \n",
    "    # detect the substrate if it is in ['PETG', 'PDMS', 'Quartz', 'SiO2', 'PC'] \n",
    "    substrates = ['PETG', 'PDMS', 'Quartz', 'SiO2', 'PC']\n",
    "    if any(s in file_name for s in substrates):\n",
    "        substrate = [s for s in substrates if s in file_name][0] # first instance\n",
    "    else :\n",
    "        substrate = 'Air'\n",
    "    \n",
    "    if layer_no:\n",
    "        # import material thickness    \n",
    "        print('%s layer %s' %(layer_no, material))\n",
    "\n",
    "        t_layered_pd, t_layered, t_layered_labels = import_data(file_name='Layered_thickness_database', data_type= \".csv\", delimiter=';', header_rows=1)\n",
    "        t_sample = t_layered_pd[material][layer_no-1]\n",
    "        print('%s nm thick %s %s' %(t_sample, thickness, material))\n",
    "    \n",
    "    data_pd= {}; data = {}; data_labels = {}\n",
    "    layers = {}; diameters = {}; pressures = {}; flakes = {}\n",
    "    \n",
    "    if file_labels == '': # single file to import\n",
    "        file_labels =['temp'] # just a dummy dictionary\n",
    "        \n",
    "    \n",
    "    for file_label in file_labels:\n",
    "        # Locate the data folder,name,type\n",
    "        if file_label == 'temp':\n",
    "            data_address = input_folder_address+\"\\\\\"+file_name+data_type\n",
    "        else:\n",
    "            data_address = input_folder_address+\"\\\\\"+file_name+file_label+data_type\n",
    "\n",
    "        # create a header array if more than 1 header rows\n",
    "        if header_rows > 1:        \n",
    "            header= np.arange(0,header_rows,1).tolist()\n",
    "        elif header_rows == 1:\n",
    "            header=0\n",
    "        else:\n",
    "            header = None\n",
    "\n",
    "        #Import the data\n",
    "        data_pd[file_label] = pd.read_csv(data_address, delimiter=delimiter, header=header)\n",
    "        if skip_columns != -1:\n",
    "            data_pd[file_label].drop(data_pd[file_label].columns[skip_columns], axis=1, inplace=True)\n",
    "\n",
    "        # See if the last column is NaN\n",
    "        if pd.isnull(data_pd[file_label].iloc[0,-1]): # first row of the last column \n",
    "            data_pd[file_label].drop(data_pd[file_label].keys()[-1], axis=1, inplace=True) # to drop the last column since it is 'NaN'\n",
    "\n",
    "        if header_rows>1:\n",
    "            data_pd[file_label].rename(columns=lambda x: x.strip(),inplace=True) # to remove the extra spaces in the labels such as ' s0 ' to 's0'\n",
    "        \n",
    "        if numeric:\n",
    "            # Convert the pd to an np array\n",
    "            print(data_pd[file_label].values)\n",
    "            data[file_label] = data_pd[file_label].values.astype(np.float)\n",
    "        else : \n",
    "            data[file_label] = data_pd[file_label].values\n",
    "\n",
    "        data_labels[file_label] = list(data_pd[file_label].columns.get_level_values(0))\n",
    "        #print(data_labels[file_label])\n",
    "\n",
    "        for header_row in range(1, header_rows):\n",
    "            data_labels[file_label]=list(map(str.__add__, data_labels[file_label], list( '_' + data_pd[file_label].columns.get_level_values(header_row) )))\n",
    "    \n",
    "        #print(data_labels[file_label])\n",
    "        \n",
    "        layers[file_label] = []; diameters[file_label] = []; pressures[file_label] = []; flakes[file_label] = []\n",
    "\n",
    "        # Pressure measurements\n",
    "        # Detect the diameter of the hole,applied pressure, flake_label(such as 4a from _#4a)\n",
    "        for label in data_labels[file_label]:\n",
    "            \n",
    "            if file_label != 'temp': # multiple files which share the same properties, but of which columns are not labelled properly in\n",
    "                label+=file_name\n",
    "            \n",
    "            layer_regex = re.compile(r'\\d+L_')\n",
    "            layer = int(re.findall( '(.*?)L_' , layer_regex.findall(label)[0])[0]) if layer_regex.findall(label) else 1 # probably a 1L\n",
    "            layers[file_label].append(layer)\n",
    "            \n",
    "            diameter_regex = re.compile(r'(?:_\\d+_micron_|_\\d+micron_)') # match both _12_micron OR _12micron_\n",
    "            #print ('label:' + label)\n",
    "            #print (diameter_regex.findall(label))\n",
    "            if diameter_regex.findall(label):\n",
    "                diameter_tuple = re.findall( '(?:_(.*?)_micron_|_(.*?)micron_)' , diameter_regex.findall(label)[0])[0]\n",
    "                #print (re.findall( '(?:_(.*?)_micron_|_(.*?)micron_)' , diameter_regex.findall(label)[0])[0])\n",
    "                diameter = int(diameter_tuple[0]+diameter_tuple[1])\n",
    "            else :\n",
    "                diameter = 0\n",
    "            diameters[file_label].append(diameter)\n",
    "            \n",
    "            pressure_regex = re.compile(r'_\\d+psi_')\n",
    "            pressure = int(re.findall( '_(.*?)psi_' , pressure_regex.findall(label)[0])[0]) if pressure_regex.findall(label) else 0\n",
    "            pressures[file_label].append(pressure)\n",
    "            \n",
    "            flake_regex = re.compile(r'_#\\w+')\n",
    "            flake = re.findall( '(?:_(#.*?)_|_(#.+))' , flake_regex.findall(label)[0])[0] if flake_regex.findall(label) else ''\n",
    "            flakes[file_label].append(''.join(flake))\n",
    "\n",
    "\n",
    "        data_labels[file_label] = [label.replace('.csv', '').replace('.asc', '').replace('.txt', '').replace('micron', r'${\\rm \\mu}$m').replace('\\g(m)', r'${\\rm \\mu}$').replace('R/R\\-(quartz)', r'R/R$_{\\rm Quartz}$').replace('___','_').replace('__','_') for label in data_labels[file_label]]\n",
    "        \n",
    "    if file_label == 'temp' : # single file imported, return the single element of the dictionaries\n",
    "        \n",
    "        # return layer_no, thickness, material, t_sample only if they are not empty\n",
    "        if layer_no :\n",
    "            print ('Date: %s' %date)\n",
    "            print ('Flakes: %s' %flakes[file_label])\n",
    "            print ('Layers: %s' %layers[file_label])\n",
    "            print ('Diameters: %s' %diameters[file_label])\n",
    "            print ('Pressures: %s' %pressures[file_label])\n",
    "            print ('Substrate: %s' %substrate)\n",
    "            \n",
    "\n",
    "            return  data_pd[file_label], data[file_label], data_labels[file_label], date, layer_no, thickness, material, t_sample, layers[file_label], diameters[file_label], pressures[file_label], flakes[file_label], substrate #if layer_no \n",
    "        else :\n",
    "            return data_pd[file_label], data[file_label], data_labels[file_label] \n",
    "    else : # multiple files imported, return them all\n",
    "        \n",
    "        \n",
    "        if layer_no :\n",
    "            print('%s nm thick %s %s' %(t_sample, thickness, material))\n",
    "            print ('Date: %s' %date)\n",
    "            print ('Flakes: %s' %flakes[file_label])\n",
    "            print ('Layers: %s' %layers[file_label])\n",
    "            print ('Diameters: %s' %diameters[file_label])\n",
    "            print ('Pressures: %s' %pressures[file_label])\n",
    "            print ('Substrate: %s' %substrate)\n",
    "            \n",
    "        if concatenate:\n",
    "            data_pd, data, data_labels, flakes, layers, diameters, pressures = concatenate_pd(data_pd, data_labels,flakes,layers,diameters,pressures)\n",
    "            \n",
    "            return data_pd, data, data_labels, date, layer_no, thickness, material, t_sample, layers, diameters, pressures, flakes, substrate\n",
    "        \n",
    "        if layer_no :\n",
    "            return data_pd, data, data_labels, date, layer_no, thickness, material, t_sample, layers[file_label], diameters[file_label], pressures[file_label], flakes[file_label], substrate\n",
    "        \n",
    "        return data_pd, data, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAY NOT BE IN USE\n",
    "def import_data2(input_folder_address=input_folder_address, file_name=def_file_name ,file_labels=file_labels, data_type= \".txt\", delimiter=',', header_rows=3):\n",
    "    \n",
    "    # Locate the datrta folder,name,type\n",
    "    data_address = input_folder_address+\"\\\\\"+file_name+data_type\n",
    "\n",
    "    # create a header array if more than 1 indices\n",
    "    if header_rows > 1:        \n",
    "        header= np.arange(0,header_rows,1).tolist()\n",
    "    elif header_rows == 1:\n",
    "        header=1\n",
    "    else:\n",
    "        header = None\n",
    "        \n",
    "    #Import the data\n",
    "    data_pd = pd.read_csv(data_address, delimiter=delimiter, header=header)\n",
    "    \n",
    "    # See if the last column is NaN\n",
    "    if pd.isnull(data_pd.iloc[0,-1]): # first row of the last column \n",
    "        data_pd = data_pd.drop(data_pd.keys()[-1], axis=1) # to drop the last column since it is 'NaN'\n",
    "\n",
    "    #data_pd.columns = [str(col) +'_'+ data_pd.ix[1,col] for col in data_pd.columns] #combine the names with comments in the imported data\n",
    "    #data_pd = data_pd.drop(1, axis=0) # Drop the third row which are characters not digits in some cases\n",
    "    if header_rows>1:\n",
    "        data_pd.rename(columns=lambda x: x.strip(),inplace=True) # to remove the extra spaces in the labels such as ' s0 ' to 's0'\n",
    "    # Convert the pd to a np array\n",
    "    data= data_pd.values.astype(np.float)\n",
    "    #row,column=data.shape\n",
    "    if header_rows>1:\n",
    "        data_labels=list(data_pd.columns.get_level_values(0) + '_' + data_pd.columns.get_level_values(header_rows-1))\n",
    "    else:\n",
    "        data_labels=list(data_pd.columns.get_level_values(0))    \n",
    "    del data_labels[:1] # to remove the fist columnn label (it is probably Energy or Wavelength)\n",
    "    return data_pd, data, data_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Concatenate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concatenate_pd(data_pd,data_labels,flakes,layers,diameters,pressures) :\n",
    "    x_first=[]; x_last=[]\n",
    "    for file_label in data_pd.keys(): # append the first and last x-axis values for comparing the x-axes\n",
    "        x_first.append(data_pd[file_label].iloc[0,0])\n",
    "        x_last.append(data_pd[file_label].iloc[-1,0])\n",
    "    \n",
    "    # do the data files share the same x-range? probably if the first and last x-axis values are the same\n",
    "    if x_first.count(x_first[0]) == len(x_first) and x_last.count(x_last[0]) == len(x_last):\n",
    "        \n",
    "        data_labels_all =[]; flakes_all=[]; layers_all=[] ; diameters_all=[]; pressures_all=[]\n",
    "        data_labels_all.append(data_labels[file_label][0]) # x-axis label, unit and y-axis label of a dataset\n",
    "        flakes_all.append(flakes[file_label][0]) # x-axis label, unit and y-axis label of a dataset\n",
    "        layers_all.append(layers[file_label][0]) # x-axis label, unit and y-axis label of a dataset\n",
    "        diameters_all.append(diameters[file_label][0]) # x-axis label, unit and y-axis label of a dataset\n",
    "        pressures_all.append(pressures[file_label][0]) # x-axis label, unit and y-axis label of a dataset\n",
    "\n",
    "        data_pd_all = data_pd[file_label].iloc[:,0] # x-axis of a data_pd\n",
    "        list_data_pd = [data_pd_all]\n",
    "\n",
    "        for file_label in file_labels:\n",
    "            list_data_pd.append(data_pd[file_label].iloc[:,1:])# excluding the x-axis\n",
    "            data_labels_all.extend(data_labels[file_label][1:])\n",
    "            flakes_all.extend(flakes[file_label][1:])\n",
    "            layers_all.extend(layers[file_label][1:])\n",
    "            diameters_all.extend(diameters[file_label][1:])\n",
    "            pressures_all.extend(pressures[file_label][1:])\n",
    "        \n",
    "        data_pd_all = pd.concat(list_data_pd, axis=1) # concatenate\n",
    "        # Convert the pd to a np array\n",
    "        data_all = data_pd_all.values.astype(np.float)\n",
    "    \n",
    "    else :\n",
    "        raise ValueError('Dataframes do not have the same x-axis')\n",
    "\n",
    "    return data_pd_all, data_all, data_labels_all, flakes_all, layers_all, diameters_all, pressures_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_2019_12_16 at 10-58'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time.strftime(\"_%Y_%m_%d at %H-%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export a pandas data frame\n",
    "def export_pd(output_pd, file_name=\"out\", output_folder_address=output_folder_address, data_type=data_type ,index=False , header=False, delimeter=',' , date_label= time.strftime(\"_%Y_%m_%d\") ):\n",
    "    output_address = output_folder_address+\"\\\\\"+file_name + date_label + data_type\n",
    "    \n",
    "    # Make sure that the file name is unique to avoid overwriting another file, otherwise add a random integer to the file name\n",
    "    if os.path.exists(output_address):\n",
    "        print('\\'' + output_address +'\\' already exists, will try adding a random int to file name')\n",
    "        output_address += '_Random # ' + str(round(random.uniform(1, 10000),1))\n",
    "\n",
    "        if os.path.exists(output_address):\n",
    "            print('\\'' + output_address +'\\' already exists, will try adding a random int to file name')\n",
    "            output_address += '_Random # ' + str(round(random.uniform(1, 10000),1))\n",
    "                \n",
    "    output_pd.to_csv(path_or_buf=output_address, index=index, header=header, sep = delimeter)\n",
    "    \n",
    "    return output_address # it is useful for finding the file such as to attach to an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_folder_address' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-36a99bbdd0d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Export a numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mexport_np\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"out\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder_address\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_folder_address\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0moutput_address\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfolder_address\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\\\\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_address\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output_folder_address' is not defined"
     ]
    }
   ],
   "source": [
    "# Export a numpy array\n",
    "def export_np(output_np, file_name=\"out\", output_folder_address=output_folder_address, delimiter=\",\", header=''):\n",
    "    output_address = output_folder_address+\"\\\\\"+file_name+data_type\n",
    "    np.savetxt(output_address, output_np, delimiter=delimiter, header=header, comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Report in Pandas Dataframe Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pd(result, data_labels = '', header=''):\n",
    "    fit_report = pd.DataFrame(index=data_labels)\n",
    "\n",
    "    for j in range(len(result[0])):\n",
    "        fit_report[header[j]] = [item[j] for item in result]\n",
    "\n",
    "    fit_report.columns=fit_report.columns.str.replace('#','')\n",
    "    return fit_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
